---
title: "Efficient Markov Chain Monte Carlo in R"
author: "Jonny Law"
date: '2019-02-11'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
library(tidyverse)
library(ggthemes)
library(ggmcmc)
library(coda)
library(parallel)
library(Rcpp)
library(microbenchmark)

theme_set(theme_minimal())
```

# Bivariate Normal Model

This post considers how to implement a simple Metropolis scheme to determine the parameter posterior distribution of a bivariate Normal distribution. The implementation is generic, using higher-order-functions and hence can be re-used with new algorithms by specifying the un-normalised log-posterior density and a proposal distribution for the parameters. The built-in `parallel` package is used fit multiple chains in parallel, finally the Metropolis algorithm is reimplemented in C++ using Rcpp which seemlessly integrates with R.

```{r}
bivariate_normal <- function(theta, n) {
  mu1 <- theta[1]
  sigma1 <- theta[2]
  mu2 <- theta[3]
  sigma2 <- theta[4]
  x <- rnorm(n / 2, mean = mu1, sd = sigma1)
  y <- rnorm(n / 2, mean = mu2, sd = sigma2)
  tibble(x, y)
}

theta = c(5.0, 0.5, 2.0, 1.5)
sims = bivariate_normal(theta, 1000)
qplot(sims$x, sims$y)
```

The random variables in the model can be written as:

$$p(y,\mu, \Sigma) = p(\mu)p(\Sigma)\prod_{i=1}^N\mathcal{N}(y_i;\mu, \Sigma)$$

The covariance matrix is diagonal, hence the log-likelihood can be written as the sum of two univariate normal distributions:

$$\log p(y|\mu, \Sigma) = \sum_{j=1}^2\left(-\frac{N}{2}\log(2\pi\sigma_j^2) - \frac{1}{2\sigma_j^2}\sum_{i=1}^N(y_{ij}-\mu_j)^2\right)$$

```{r}
log_likelihood <- function(xs, theta) {
  apply(xs, 1, function(x) dnorm(x[1], mean = theta[1], sd = theta[2], log = T) + 
          dnorm(x[2], mean = theta[3], sd = theta[4], log = T)) %>% sum()
}
```

The prior distributions are chosen to be:

$$\begin{align}
p(\mu_j) &= \mathcal{N}(0, 3), \\
p(\Sigma_{jj}) &= \textrm{Gamma}(3, 3), \quad j = 1, 2.
\end{align}$$

```{r}
log_prior <- function(theta) {
  dnorm(theta[1], log = T) + 
    dnorm(theta[3], log = T) + 
    dgamma(theta[2], shape = 3.0, rate = 3.0, log = T) + 
    dgamma(theta[4], shape = 3.0, rate = 3.0, log = T)
}
log_posterior <- function(xs) 
  function(theta) 
    log_likelihood(xs, theta) + log_prior(theta)
```

# Metropolis algorithm in R

A [Metropolis](https://en.wikipedia.org/wiki/Metropolisâ€“Hastings_algorithm) algorithm can be used to determine the posterior distribution of the parameters, $\mu$ and $\Sigma$.

```{r}
metropolis_step <- function(theta, log_posterior, proposal) {
  propTheta <- proposal(theta)
  a <- log_posterior(propTheta) - log_posterior(theta)
  u <- runif(1)
  if (log(u) < a) {
    propTheta
  } else {
    theta
  }
}

metropolis <- function(theta, log_posterior, proposal, m) {
  out = matrix(NA_real_, nrow = m, ncol = length(theta))
  out[1, ] = theta
  for (i in 2:m) {
    out[i, ] <- metropolis_step(out[i-1, ], log_posterior, proposal)
  }
  out
}
```

The strictly positive variance parameters are proposed on the log-scale:

```{r run-metropolis}
proposal <- function(x) {
  z = rnorm(4, sd = 0.05)
  c(x[1] + z[1], x[2] * exp(z[2]),
    x[3] + z[3], x[4] * exp(z[4]))
}
```

Finally, all the components are there to sample from the posterior distribution of the parameters. The mean of the sampled posterior distribution should coincide with the parameters used to simulate the data. In the figure below the actual values used to simulate the data are plotted with dashed lines.

```{r}
xs <- as.matrix(sims)
out = metropolis(theta, log_posterior(xs), proposal, 10000)
```

```{r, echo=FALSE}
actual_values = tibble(
  Parameter = c("mu1", "sigma1", "mu2", "sigma2"),
  actual_value = c(5.0, 0.5, 2.0, 1.5)
)

out <- as_tibble(out)
colnames(out) <- actual_values$Parameter

out %>%
  mutate(iteration = row_number()) %>%
  gather(key = Parameter, value, -iteration) %>%
  inner_join(actual_values) %>%
  ggplot(aes(x = iteration, y = value)) +
  geom_line(alpha = 0.5) +
  geom_hline(aes(yintercept = actual_value), linetype = "dashed") +
  facet_wrap(~Parameter, scales = "free_y")
```

# Parallel Chains

Typically, multiple chains are run in parallel, a straightforward way to do this in R is to use a parallel version of `lapply`. The following exports the functions and data required by the Metropolis algorithm to the "cluster" which are simply multiple cores on the host machine and runs two MCMC algorithms in parallel.

```{r}
cl <- makeCluster(2)
clusterExport(cl, varlist = c("metropolis", "theta", "log_posterior", "xs", "proposal", "metropolis_step", "log_likelihood", "log_prior"))
clusterEvalQ(cl, library(dplyr))
chains = parLapply(cl, 1:2, function(x) metropolis(theta, log_posterior(xs), proposal, 10000))
stopCluster(cl)
```

The figure below shows the trace plots from 10,000 draws of the Metropolis hastings algorithm.

```{r parallel-diagnostics, echo=FALSE}
mh_samples <- chains %>%
  map(mcmc) %>%
  mcmc.list() %>%
  ggs()

ggs_traceplot(mh_samples)
ggs_autocorrelation(mh_samples)
ggs_density(mh_samples)
```

# Rcpp implementation

R has a straightforward interface to C++, the Metropolis-Hastings algorithm can be re-implemented using C++. C++ is a statically typed imperative language, hopefully the effort of reimplementing in C++ will result in a significant speed-up. The following code-chunk shows how the Metropolis algorithm is implemented in C++. I don't have much experience with C++ (in fact this is the first time I've written anything in C++!) so there are most likely better ways to write this, for instance using templating and generic implementations which would make it easier to re-use.

```{Rcpp}
#include <Rcpp.h>
using namespace Rcpp;
// [[Rcpp::plugins(cpp11)]]

// [[Rcpp::export]]
double logDensity(NumericMatrix ys, NumericVector p) {
  double ll = 0;
  int n = ys.nrow();
  for (int i = 0; i < n; i++) {
    ll += R::dnorm(ys(i, 0), p(0), p(1), true) + R::dnorm(ys(i, 1), p(2), p(3), true);
  }
  return ll;
}

// [[Rcpp::export]]
NumericVector proposalCpp(NumericVector p, double delta) {
  int d = p.size();
  NumericVector z(d);
  NumericVector propP(d);
  for (int i = 0; i < d; i++) {
    propP(i) = p(i);
    z(i) = R::rnorm(0, delta);
  }
  propP(0) += z(0);
  propP(1) *= exp(z(1));
  propP(2) += z(2);
  propP(3) *= exp(z(3));
  return propP;
}

// [[Rcpp::export]]
double logAcceptance(NumericMatrix ys,
                      NumericVector propP, 
                      NumericVector p) {
  double a = logDensity(ys, propP) - logDensity(ys, p);
  return a;
}

// [[Rcpp::export]]
NumericVector mStep(NumericMatrix ys, NumericVector p) {
  NumericVector propP = proposalCpp(p, 0.05);
  double a = logAcceptance(ys, propP, p);
  double u = R::runif(0, 1);
  NumericVector nextParameter;
  if (log(u) < a) {
    nextParameter = propP;
  } else {
    nextParameter = p;
  }

  return nextParameter;
}

// [[Rcpp::export]]
NumericMatrix metropolisCpp(NumericMatrix ys, 
                            NumericVector p,
                            int N) {
  int d = p.size();
  NumericMatrix mat(N, d);
  // initialise first row of output
  for (int j = 0; j < d; j++) {
    mat(0, j) = p(j);
  }
  
  // create markov chain
  for (int i = 1; i < N; i++) {
    NumericVector newP = mStep(ys, mat(i-1, _));
    for (int j = 0; j < d; j++) {
      mat(i, j) = newP(j);
    }
  }
  return mat;
}
```

The algorithm is run using the C++ implementation and the results plotted using R, as we can see from the below code chunk the C++ function appears as if it was an R function but performs much quicker!

```{r}
out_cpp <- metropolisCpp(xs, theta, 10000)

out_cpp %>%
  mcmc() %>%
  ggs() %>%
  ggs_traceplot()
```

The relative speedup can be calculated using the [https://github.com/joshuaulrich/microbenchmark/](microbenchmark) package. 
The relative speedup is plotted below 100 correlated samples from the posterior distribution.


```{r}
timings <- microbenchmark(metropolis(theta, log_posterior(xs), proposal, 100), 
                          metropolisCpp(xs, theta, 100))
```

```{r relative-speedup, echo=FALSE}
timings %>% 
  as.data.frame() %>%
  mutate(expr = ifelse(grepl("Cpp", x = expr), "C++", "R")) %>% 
  group_by(expr) %>%
  mutate(iteration = 1:n()) %>%
  spread(expr, time) %>%
  mutate(relative_speedup = R / `C++`) %>%
  ggplot(aes(x = relative_speedup)) +
  geom_histogram()
```

