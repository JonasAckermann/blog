---
title: "Scala HMC"
author: "Jonny Law"
date: "2019-04-30"
output: html_document
draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Scala implementation

To implement HMC, first identify the individual components of the algorithm and
write them as referentially transparent functions. The first step is to sample
the momentum, $\phi$ from the prior distribution, chosen to be a standard multivariate Normal distribution of dimension $m$:

```scala
def priorPhi(m: Int) = 
  MultivariateGaussian(DenseVector.zeros[Double](m),
    DenseMatrix.eye[Double](m))
```

The prior distribution of $\phi$ represented by the function
`priorPhi` can be sampled from and the log-pdf can be calculated.
The prior distribution can have a general covariance matrix $\Sigma$ to replace
the $m$ dimensional identity matrix $I_m$. Next the leapfrog discretization of
Hamilton's equations can be implemented:

```scala
def leapfrog(
  eps: Double,
  gradient: DenseVector[Double] => DenseVector[Double])(
  psi: DenseVector[Double],
  phi: DenseVector[Double]) = {
  val p1 = phi + eps * 0.5 * gradient(psi)
  val t1 = psi + eps * p1
  val p2 = p1 + eps * 0.5 * gradient(t1)
  (t1, p2)
}
```

The parameters and momentum are represented as `DenseVector`s from the
Breeze scientific computing library to simplify
the implementation details, since addition and multiplication of vectors are
already implemented. A function which performs multiple
`leapfrog` steps dependent on the previous value can be implemented
using `Vector.iterate`. Finally the HMC kernel can be written as a function from the parameters to a distribution over the new parameters, `DenseVector[Double] => Rand[DenseVector[Double]`:

```scala
def step(
  priorPhi: ContinuousDistr[Double],
  eps: Double,
  l: Int,
  gradient: DenseVector[Double] => DenseVector[Double],
  logPos: DenseVector[Double] => Double
  )(psi: DenseVector[Double]): Rand[DenseVector[Double]] = {
  for {
    phi <- priorPhi
    (propPsi, propPhi) = leapfrogs(eps, gradient, l, psi, phi)
    a = logPos(propPsi) - priorPhi.logPdf(propPhi) -
      logPos(psi) + priorPhi.logPdf(phi)
    u <- Uniform(0, 1)
    next = if (log(u) < a) {
      propPsi
    } else {
      psi
    }
  } yield next
}
```

The kernel can then be used to draw samples
from a posterior distribution of a model with un-normalised log-posterior
`logPos` and gradient, `grad` using `MarkovChain`:

```scala

```
