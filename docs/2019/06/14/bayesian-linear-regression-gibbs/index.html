<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.55.1" />


<title>Bayesian Linear Regression with Gibbs Sampling in R - Bayesian Statistics and Functional Programming </title>
<meta property="og:title" content="Bayesian Linear Regression with Gibbs Sampling in R - Bayesian Statistics and Functional Programming ">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/tomorrow-night-eighties.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/picture_cropped.jpg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/">Blog</a></li>
    
    <li><a href="https://github.com/jonnylaw">GitHub</a></li>
    
    <li><a href="https://twitter.com/lawsy">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">7 min read</span>
    

    <h1 class="article-title">Bayesian Linear Regression with Gibbs Sampling in R</h1>

    
    <span class="article-date">2019/06/14</span>
    

    <div class="article-content">
      


<p>Linear regression models are commonly used to explain relationships between predictor variables and outcome variables. The data consists of pairs of independent observations <span class="math inline">\((y_i, x_i)\)</span> where <span class="math inline">\(y_i \in \mathbb{R}^p\)</span> represents the outcome variable of the <span class="math inline">\(i^{th}\)</span> observation and <span class="math inline">\(x_i \in \mathbb{R}^{m \times 1}\)</span> represents the predictor variable of the <span class="math inline">\(i^{th}\)</span> observation. The specification for this model is:</p>
<p><span class="math display">\[y_i = \beta^T x_i + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0, \Sigma).\]</span></p>
<p>The parameters of the model include the coefficients of the predictor variables, <span class="math inline">\(\beta \in \mathbb{R}^{1 \times m}\)</span> and the variance of the unmodelled noise, <span class="math inline">\(\Sigma \in \mathbb{R}^{p \times p}\)</span>.</p>
<div id="the-model-as-a-data-generating-process" class="section level2">
<h2>The Model as a Data Generating Process</h2>
<p>In order to manufacture a deeper understand of linear regression it is useful to explore the model as a data generating process. This allows us to understand when linear regression is applicable, how to effectively perform parameter inference and how to assess the model fit. If the model is suitable for the application, then synthetic data from the model with appropriately chosen parameters should be indistinguishable from real observed data. The parameters used to generate the simulated data are known and hence inference algorithms should be able to reliably recover these parameters using the simulated data.</p>
<p>First consider a simple linear regression (a regression where there is only one predictor variable) which links height to weight. We assume that height will be of adults and measured in cm. This is a continuous variable and we might think that this could be modelled using a Normal distribution with a mean of <span class="math inline">\(150\)</span> and a standard deviation of <span class="math inline">\(20\)</span>. Let’s simulate some values:</p>
<pre class="r"><code>heights &lt;- rnorm(100, mean = 150, sd = 20)
qplot(heights, geom = &quot;histogram&quot;)</code></pre>
<p><img src="/post/2019-06-14-bayesian-linear-regression_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>We have simulated 100 heights and plotted them on a histogram. The tallest adults are 200cm and the smallest are 100cm. Now that we have our heights, it remains to choose a suitable value for the parameter <span class="math inline">\(\alpha\)</span> which will be the intercept and the coefficient <span class="math inline">\(\beta\)</span> which will be multiplied by height to determine the weight in kilograms. In addition, a value of the unmodelled noise <span class="math inline">\(\sigma\)</span> must be chosen, this seems reasonable since we know that other factors apart from height determine an individuals weight.</p>
<pre class="r"><code>alpha &lt;- 60
beta &lt;- 0.3
sigma &lt;- 5
weights &lt;- purrr::map_dbl(heights, ~ rnorm(1, mean = alpha + beta * ., sd = sigma))
qplot(weights, geom = &quot;histogram&quot;)</code></pre>
<p><img src="/post/2019-06-14-bayesian-linear-regression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>For every height, we have simulated an associated weight using <a href="https://purrr.tidyverse.org">purrrs</a> <code>map_dbl</code>. We can plot the height against the weight and see that there is a generally increasing trend, this is expected since our chosen value of the coefficient <span class="math inline">\(\beta = 0.5\)</span>.</p>
<pre class="r"><code>qplot(heights, weights)</code></pre>
<p><img src="/post/2019-06-14-bayesian-linear-regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>When performing an applied analysis in a business context, it might be tempting to stop here after plotting the relationship between height and weight. However these heights and weights are only a sample of a population - we wish to make statements which pertain to the entire population. If we consider the sample representative of the population then a properly fitted statistical model will allow us to make statements about the population which this sample is drawn from. As an example of a common business problem, this could include sales of a product - we wish to make statements about future sales which we can’t possibly have seen and hence a statistical model is important.</p>
</div>
<div id="fitting-the-model" class="section level2">
<h2>Fitting the model</h2>
<p>A parametric model is described by a distribution <span class="math inline">\(p(y|\theta)\)</span> where <span class="math inline">\(y\)</span> represents the observed data and <span class="math inline">\(\theta\)</span> represents the parameters. These parameters are unknown, but represent properties of the model. The distribution of the observed data is controlled by the values of the parameters, <span class="math inline">\(\theta\)</span>. The goal of Bayesian inference is to learn which values of the parameters are consistent with the observed data. The parameters are unknown and can’t be determined precisely, however the more data collected the more accurate the posterior inferences can be.</p>
<p>In the Bayesian paradigm, the parameters also have a distribution. Before the data is observed, this is referred to as the prior distribution <span class="math inline">\(p(\theta)\)</span> which can incorporate the hypothesis of the analyst. The goal is to determine the posterior distribution of the parameters given the observed data, this can be achieved using Bayes theorem:</p>
<p><span class="math inline">\(p(\theta|y) = \frac{p(\theta)p(y|\theta)}{\int_\theta p(\theta)p(y|\theta)d\theta}\)</span></p>
<p>The likelihood for linear regression with <span class="math inline">\(n\)</span> univariate observations, <span class="math inline">\(\textbf{y} = y_1,\dots,y_n\)</span> is written as</p>
<p><span class="math display">\[p(\textbf{y}|\psi) = \prod_{i=1}^n\mathcal{N}(\beta^Tx_i, \tau),\]</span></p>
<p>note that the likelihood is parameterised in terms of the precision <span class="math inline">\(\tau = \frac{1}{\sigma^2}\)</span>. Standard prior distributions for simple linear regression are chosen to be</p>
<p><span class="math display">\[\begin{align*}
p(\tau) &amp;= \textrm{Gamma}(\alpha_\sigma, \beta_\sigma), \\
p(\alpha) &amp;= \mathcal{N}(\mu_\alpha, \sigma^2_\alpha), \\
p(\beta) &amp;= \mathcal{N}(\mu_\beta, \sigma^2_\beta).
\end{align*}\]</span></p>
<div id="gibbs-sampling" class="section level3">
<h3>Gibbs Sampling</h3>
<p>Gibbs sampling works by alternately sampling from the conditional conjugate distribution. It can often be faster for models which are specified using the conjugate structure, however the choice of prior distribution is not flexible (but the parameterisation is). The algebra below is not required to implement a Gibbs sampling algorithm as there are probabilistic programming languages such as BUGS and JAGS which work out the required maths.</p>
<p>Using the likelihood and priors from the section above we can derive the conditionally conjugate posterior distributions:</p>
<p><span class="math display">\[\begin{align*}
p(\tau|\textbf{y}, \textbf{x}, \psi^{(-\tau)}) &amp;= p(\tau)\prod_{i=1}^np(y_i|\psi), \\
&amp;= \textrm{Gamma}(\tau|\alpha_\sigma, \beta_\sigma)\prod_{i=1}^n\mathcal{N}(y_i|\beta^Tx_i, \sigma^2), \\
&amp;\propto \tau^{\alpha_\tau-1}e^{-\beta_\tau\tau}\tau^{\frac{n}{2}}\exp\left\{-\frac{\tau}{2}\sum_{i=1}^n(y_i-\beta^Tx_i)^2\right\}, \\
&amp;= \tau^{\alpha_\tau-1 + \frac{n}{2}}\exp\left\{-\beta_\tau\tau-\frac{\tau}{2}\sum_{i=1}^n(y_i-\beta^Tx_i)^2\right\},\\
&amp;=\textrm{Gamma}\left(\alpha_\tau+\frac{n}{2}, \beta_\tau +\frac{1}{2}\sum_{i=1}^n(y_i-\alpha - \beta x_i)^2\right).
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
p(\alpha|\textbf{y}, \textbf{x}, \beta, \tau) &amp;= \mathcal{N}(\alpha|\mu_\alpha, \tau_\alpha)\prod_{i=1}^n\mathcal{N}(y_i|\alpha + \beta x_i, \tau), \\
&amp;\propto \tau^{\frac{1}{2}}_\alpha\exp\left\{-\frac{\tau_\alpha}{2}(\alpha-\mu_\alpha)^2\right\}\tau^\frac{n}{2}\exp\left\{-\frac{\tau}{2}\sum_{i=1}^n(y_i-\alpha-\beta x_i)^2\right\}, \\
&amp;= \exp\left\{-\frac{\tau_\alpha}{2}(\alpha-\mu_\alpha)^2-\frac{\tau}{2}\sum_{i=1}^n(y_i-\alpha-\beta x_i)^2\right\}, \\
&amp;= \exp \left\{ -\frac{1}{2}\left(\alpha^2(\tau_\alpha + n\tau) + \alpha(-2\tau_\alpha\mu_\alpha - 2\tau\sum_{i=1}^n (y_i - \beta x_i)) \right) + C \right\}, \\
&amp;= \mathcal{N}\left((\tau_\alpha + n\tau)^{-1}\left(\tau_\alpha + \tau\sum_{i=1}^n (y_i - \beta x_i)\right), \tau_\alpha + n\tau\right).
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
p(\beta|\textbf{y}, \textbf{x}, \beta, \tau) &amp;= \mathcal{N}(\beta|\mu_\beta, \tau_\beta)\prod_{i=1}^n\mathcal{N}(y_i|\alpha + \beta x_i, \tau), \\
&amp;\propto \tau^{\frac{1}{2}}_\beta\exp\left\{-\frac{\tau_\beta}{2}(\beta-\mu_\beta)^2\right\}\tau^\frac{n}{2}\exp\left\{-\frac{\tau}{2}\sum_{i=1}^n(y_i-\alpha-\beta x_i)^2\right\}, \\
&amp;= \exp\left\{-\frac{\tau_\beta}{2}(\beta-\mu_\beta)^2-\frac{\tau}{2}\sum_{i=1}^n(y_i-\alpha-\beta x_i)^2\right\}, \\
&amp;= \exp \left\{ -\frac{1}{2}\left(\beta^2(\tau_\beta + \tau\sum_{i=1}^nx_i^2) + \beta(-2\tau_\beta\mu_\beta - 2\tau\sum_{i=1}^n (y_i - \alpha) x_i) \right) + C \right\}, \\
&amp;= \mathcal{N}\left((\tau_\beta + \sum_{i=1}^nx_i^2\tau)^{-1}\left(\tau_\beta + \tau\sum_{i=1}^n (y_i - \alpha )x_i\right), \tau_\beta + \tau\sum_{i=1}^nx_i^2 \right).
\end{align*}\]</span></p>
<p>This allows us to construct a Gibbs Sampler for the linear regression model by alternating sampling from the precision, <span class="math inline">\(\tau\)</span> given the latest value of the coefficient vector <span class="math inline">\(\beta\)</span> and vice versa. The functions to sample from the conditional posterior distributions are written in <code>R</code> as:</p>
<pre class="r"><code>sample_tau &lt;- function(ys, alpha, beta, alpha0, beta0) {
  rgamma(1,
         shape = alpha0 + nrow(ys) / 2,
         rate = beta0 + 0.5 * sum((ys$y - (alpha + ys$x * beta)) ^ 2))
}

sample_alpha &lt;- function(ys, beta, tau, mu0, tau0) {
  prec &lt;- tau0 + tau * nrow(ys)
  mean = (tau0 + tau * sum(ys$y - beta * ys$x)) / prec
  rnorm(1, mean = mean, sd = 1 / sqrt(prec))
}

sample_beta &lt;- function(ys, alpha, tau, mu0, tau0) {
  prec &lt;- tau0 + tau * sum(ys$x * ys$x)
  mean = (tau0 + tau * sum((ys$y - alpha) * ys$x)) / prec
  rnorm(1, mean = mean, sd = 1 / sqrt(prec))
}</code></pre>
<p>Then a function which loops through each conditional distribution in turn is defined using the three functions defined above. Each conditional distribution is dependent on the parameter draw made immediately above.</p>
<pre class="r"><code>gibbs_sample &lt;-
  function(ys,
           tau0,
           alpha0,
           beta0,
           m,
           alpha_tau,
           beta_tau,
           mu_alpha,
           tau_alpha,
           mu_beta,
           tau_beta) {
    tau = numeric(m)
    alpha &lt;- numeric(m)
    beta &lt;- numeric(m)
    tau[1] &lt;- tau0
    alpha[1] &lt;- alpha0
    beta[1] &lt;- beta0
    
    for (i in 2:m) {
      tau[i] &lt;-
        sample_tau(ys, alpha[i - 1], beta[i - 1], alpha_tau, beta_tau)
      alpha[i] &lt;- sample_alpha(ys, beta[i - 1], tau[i], mu_alpha, tau_alpha)
      beta[i] &lt;- sample_beta(ys, alpha[i], tau[i], mu_beta, tau_beta)
    }
    
    tibble(draw = seq_len(m),
           tau,
           alpha,
           beta)
  }</code></pre>
<p>It is often useful to transform the data, in this instance we use the function <code>scale</code> to centre and scale the data by subtracting the mean and dividing by the standard deviation. This can be recovered</p>
<pre class="r"><code>ys &lt;- tibble(y = scale(weights), x = scale(heights))</code></pre>
<pre class="r"><code>iters &lt;- map_dfr(
  .x = 1:2,
  .f = function(x) gibbs_sample(
    ys,
    tau0 = 0.5,
    alpha0 = 60,
    beta0 = 0.3,
    m = 10000,
    alpha_tau = 3,
    beta_tau = 2,
    mu_alpha = 0,
    tau_alpha = 0.01,
    mu_beta = 0,
    tau_beta = 0.01
  ),
  .id = &quot;chain&quot;
)</code></pre>
<pre class="r"><code>p1 &lt;- iters %&gt;% 
  filter(draw &gt; 5000) %&gt;% 
  mutate(sigma = sqrt(1 / tau)) %&gt;%
  select(-tau) %&gt;% 
  gather(key, value, -draw, -chain) %&gt;% 
  ggplot(aes(x = draw, y = value, colour = chain)) +
  geom_line() +
  facet_wrap(~key, scales = &quot;free_y&quot;)

p2 &lt;- iters %&gt;%  
  filter(draw &gt; 5000) %&gt;% 
  mutate(sigma = sqrt(1 / tau)) %&gt;%
  select(-tau) %&gt;% 
  gather(key, value, -draw, -chain) %&gt;% 
  ggplot(aes(x = value, fill = chain)) +
  geom_density(alpha = 0.5) + 
  facet_wrap(~key, scales = &quot;free&quot;)

p1 / p2</code></pre>
<p><img src="/post/2019-06-14-bayesian-linear-regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>*** Posterior fitted values</p>
<pre class="r"><code>ggplot() +
  geom_point(data = ys, aes(x = x, y = y)) +
  geom_abline(
    data = iters %&gt;%
      filter(draw &gt; 9000),
    aes(intercept = alpha, slope = beta),
    alpha = 0.02,
    colour = &quot;pink&quot;
  )</code></pre>
<p><img src="/post/2019-06-14-bayesian-linear-regression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>This can be sped up even more using <a href="/2019/02/11/efficient_mcmc_using_rcpp/">RCpp</a>.</p>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/scala.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

